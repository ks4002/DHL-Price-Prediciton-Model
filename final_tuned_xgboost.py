# -*- coding: utf-8 -*-
"""Final_tuned_xgboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fHaAmNG19_vgY9syQOhIV797BQibZYIu
"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

"""# New Section"""

#Upload 2.3 million row dataset and remove non-numeric variables 
data = pd.read_csv('FE_Done_Final_2.04M.csv')
data.drop(['Unnamed: 0.1', 'Unnamed: 0', 'PU_APPT_DATETIME', 'DL_APPT_DATETIME'], axis=1, inplace=True)

order = [4,0,1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36]
data = data[[data.columns[i] for i in order]]
data.info()

#Split data into dependent and independent variable sets
X, y = data.iloc[:,1:37], data.iloc[:,0]

print(X.info())
print(y.info())

#Transform data into matrix for efficient xgboost training 
data_dmatrix = xgb.DMatrix(data = X,label = y)

#Split data into test and training set 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)

from sklearn.model_selection import RandomizedSearchCV

#Set hyperparameter ranges, xgboost model, and cross-validated
xgboost = xgb.XGBRegressor(seed = 42)

params = { 'max_depth': [10, 15, 20, 25, 30],
           'learning_rate': [0.01, 0.05, 0.1],
           'subsample': np.arange(0.7, 1.0, 0.1),
           'colsample_bytree': np.arange(0.8, 1.0, 0.1),
           'colsample_bylevel': np.arange(0.8, 1.0, 0.1),
           'n_estimators': [500, 1000, 1500],
           'gamma': np.arange(0, 10, 1),
           'min_child_weight': np.arange(0, 10, 1)
         }

xgboost_random = RandomizedSearchCV(estimator = xgboost,
                         param_distributions = params,
                         scoring = 'neg_mean_squared_error',
                         verbose = 0,
                         cv = 7,
                         random_state = 42,
                         n_jobs = -1)                       

xgboost_random.fit(X_train, y_train)

print(xgboost_random.best_params_)

#Used best features to train model, but increased the n_estimators to 1000
xgb_cv = xgb.XGBRegressor(subsample = 0.9999999999999999, 
                          n_estimators = 1000, 
                          min_child_weight = 7, 
                          max_depth = 20, 
                          learning_rate = 0.01, 
                          gamma = 4, 
                          colsample_bytree = 0.8, 
                          colsample_bylevel = 0.9, 
                          seed = 42)
xgb_cv

#Fit model with best hyperparameters, but increase n_estimators to 1000
xgb_cv.fit(X_train,y_train)

preds = xgb_cv.predict(X_test)

#Calculated RMSE
rmse = np.sqrt(mean_squared_error(y_test, preds))

print("RMSE: %f" % (rmse))

import joblib

joblib.dump(xgb_cv, "xgb_final_model.sav")